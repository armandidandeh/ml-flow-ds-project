{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load snippet/default_notebook_setup.py\n",
    "## Not all libraries support reloads. This might break things.\n",
    "## https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "## use %autoreload 1 and %aimport project for selective reloads\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "## Load environment variables from our .env file\n",
    "#%load_ext dotenv\n",
    "#%dotenv\n",
    "\n",
    "# Add the jupyter home directory to the python path\n",
    "# to find our project code base for imports\n",
    "import sys\n",
    "sys.path.append('/home/jovyan')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load snippet/default_spark.py\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Window, DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .config('spark.jars.packages', 'ml.combust.mleap:mleap-spark-base_2.11:0.14.0,ml.combust.mleap:mleap-spark_2.11:0.14.0')\n",
    "    .config('spark.sql.execution.arrow.enabled', 'true')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport project\n",
    "from project.data import raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+---------------+--------------+-----------+\n",
      "|sepal_length_cm|sepal_width_cm|petal_length_cm|petal_width_cm|      class|\n",
      "+---------------+--------------+---------------+--------------+-----------+\n",
      "|            5.1|           3.5|            1.4|           0.2|Iris-setosa|\n",
      "+---------------+--------------+---------------+--------------+-----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris_df = raw.load_iris('../')\n",
    "iris_df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Development\n",
    "\n",
    "In a first step we use Jupyter to experiment with the data and Spark to build a functioning feature pipeline. We use all the flexibility of Notebooks here to print out the results and test the code we write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, PolynomialExpansion, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+---------------+--------------+-----------+-----------------+\n",
      "|sepal_length_cm|sepal_width_cm|petal_length_cm|petal_width_cm|      class|         features|\n",
      "+---------------+--------------+---------------+--------------+-----------+-----------------+\n",
      "|            5.1|           3.5|            1.4|           0.2|Iris-setosa|[5.1,3.5,1.4,0.2]|\n",
      "+---------------+--------------+---------------+--------------+-----------+-----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"sepal_length_cm\", \"sepal_width_cm\", \"petal_length_cm\", \"petal_width_cm\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "features = assembler.transform(iris_df)\n",
    "features.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+---------------+--------------+-----------+-----------------+-------------------------------------------------------------------------------+\n",
      "|sepal_length_cm|sepal_width_cm|petal_length_cm|petal_width_cm|class      |features         |scaledFeatures                                                                 |\n",
      "+---------------+--------------+---------------+--------------+-----------+-----------------+-------------------------------------------------------------------------------+\n",
      "|5.1            |3.5           |1.4            |0.2           |Iris-setosa|[5.1,3.5,1.4,0.2]|[-0.8976738791967643,1.0286112808972372,-1.3367940202882502,-1.308592819437957]|\n",
      "+---------------+--------------+---------------+--------------+-----------+-----------------+-------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler(\n",
    "    inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "    withStd=True, withMean=True\n",
    ")\n",
    "\n",
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel = scaler.fit(features)\n",
    "\n",
    "# Normalize each feature to have unit standard deviation.\n",
    "scaledFeatures = scalerModel.transform(features)\n",
    "scaledFeatures.show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(polyFeatures=DenseVector([-0.8977, 0.8058, -0.7234, 1.0286, -0.9234, 0.8289, 1.058, -0.9498, 1.0883, -1.3368, 1.2, -1.0772, -1.375, 1.2343, -1.4144, 1.787, -1.6042, 1.8381, -2.3889, -1.3086, 1.1747, -1.0545, -1.346, 1.2083, -1.3845, 1.7493, -1.5703, 1.7994, -2.3385, 1.7124, -1.5372, 1.7614, -2.2891, -2.2409]))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polyExpansion = PolynomialExpansion(degree=3, inputCol=\"scaledFeatures\", outputCol=\"polyFeatures\")\n",
    "polyFeatures = polyExpansion.transform(scaledFeatures)\n",
    "polyFeatures.select('polyFeatures').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Incorporate  the Feature Pipeline into the Project code base\n",
    "\n",
    "After we have functioning code for our feature pipeline, we incorporate that code into our project codebase.\n",
    "We can now use that functions and even test that both versions behave identically. In a next step we can \n",
    "\n",
    "1. Delete the previous code cells for Step 1\n",
    "1. Restart the kernel\n",
    "1. Rerun all the code cells in order to assure a clean state\n",
    "1. Continue working on the next tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project.data import features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_pipeline = features.train_new_feature_pipeline(iris_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+---------------+--------------+-----------+-----------+-----------------+--------------------+--------------------+\n",
      "|sepal_length_cm|sepal_width_cm|petal_length_cm|petal_width_cm|      class|class_index|         features|      scaledFeatures|        polyFeatures|\n",
      "+---------------+--------------+---------------+--------------+-----------+-----------+-----------------+--------------------+--------------------+\n",
      "|            5.1|           3.5|            1.4|           0.2|Iris-setosa|        0.0|[5.1,3.5,1.4,0.2]|[-0.8976738791967...|[-0.8976738791967...|\n",
      "+---------------+--------------+---------------+--------------+-----------+-----------+-----------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_pipeline.transform(iris_df).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(polyFeatures=DenseVector([-0.8977, 0.8058, -0.7234, 1.0286, -0.9234, 0.8289, 1.058, -0.9498, 1.0883, -1.3368, 1.2, -1.0772, -1.375, 1.2343, -1.4144, 1.787, -1.6042, 1.8381, -2.3889, -1.3086, 1.1747, -1.0545, -1.346, 1.2083, -1.3845, 1.7493, -1.5703, 1.7994, -2.3385, 1.7124, -1.5372, 1.7614, -2.2891, -2.2409]))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_pipeline.transform(iris_df).select('polyFeatures').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Model Experimentation with Mlflow\n",
    "\n",
    "It is important to track parameters and metrics when experimenting with models and data. __Mlflow__ is a great tool to track experimentation and save models. Let's use MLflow to save our feature pipeline which uses the Spark StandardScaler which requires fitting and is not stateless.\n",
    "\n",
    "## Saving a spark model with Mlflow\n",
    "\n",
    "Unfortunately, there are some bugs preventing boto3 to upload empty files to minio (or S3 stand-in for local development). To circumvent this problem we save the model locally with Mlflow and upload it afterwards with our __log_artifacts_minio()__ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from project.utility.mlflow import log_artifacts_minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'iris_features'\n",
    "mlflow.set_tracking_uri(os.environ['MLFLOW_TRACKING_URI'])\n",
    "mlflow.set_experiment(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<RunInfo: artifact_uri='s3://artifacts/0/f3a735a824264043a7978ee1aa0230d6/artifacts', end_time=None, experiment_id='0', lifecycle_stage='active', run_id='f3a735a824264043a7978ee1aa0230d6', run_uuid='f3a735a824264043a7978ee1aa0230d6', start_time=1572870582878, status='RUNNING', user_id='jovyan'>\n"
     ]
    }
   ],
   "source": [
    "degree = 3\n",
    "with mlflow.start_run() as run:\n",
    "    feature_pipeline = features.train_new_feature_pipeline(iris_df, degree)\n",
    "    mlflow.log_param(\"degree\", degree)\n",
    "    mlflow.spark.save_model(\n",
    "        feature_pipeline, \n",
    "        'feature_pipeline', \n",
    "        sample_input=iris_df.select(\n",
    "            'sepal_length_cm',\n",
    "            'sepal_width_cm',\n",
    "            'petal_length_cm',\n",
    "            'petal_width_cm'\n",
    "        )\n",
    "    )\n",
    "    log_artifacts_minio(run, 'feature_pipeline', 'feature_pipeline', True)\n",
    "    run_id = run.info.run_id\n",
    "    print(run.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a spark model using Mlflow\n",
    "\n",
    "We load the seriealised model with Mlflow using the previous run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runs:/f3a735a824264043a7978ee1aa0230d6/feature_pipeline\n"
     ]
    }
   ],
   "source": [
    "artifact_uri = 'runs:/{}/feature_pipeline'.format(run_id)\n",
    "model = mlflow.spark.load_model(artifact_uri)\n",
    "print(artifact_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+---------------+--------------+-----------+-----------------+--------------------+--------------------+\n",
      "|sepal_length_cm|sepal_width_cm|petal_length_cm|petal_width_cm|      class|         features|      scaledFeatures|        polyFeatures|\n",
      "+---------------+--------------+---------------+--------------+-----------+-----------------+--------------------+--------------------+\n",
      "|            5.1|           3.5|            1.4|           0.2|Iris-setosa|[5.1,3.5,1.4,0.2]|[-0.8976738791967...|[-0.8976738791967...|\n",
      "+---------------+--------------+---------------+--------------+-----------+-----------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(iris_df).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get logged metrics and parameters from the tracked run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Run: data=<RunData: metrics={}, params={'degree': '3'}, tags={'mlflow.source.name': '/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py',\n",
      " 'mlflow.source.type': 'LOCAL',\n",
      " 'mlflow.user': 'jovyan'}>, info=<RunInfo: artifact_uri='s3://artifacts/0/f3a735a824264043a7978ee1aa0230d6/artifacts', end_time=1572870584338, experiment_id='0', lifecycle_stage='active', run_id='f3a735a824264043a7978ee1aa0230d6', run_uuid='f3a735a824264043a7978ee1aa0230d6', start_time=1572870582878, status='FINISHED', user_id='jovyan'>>\n"
     ]
    }
   ],
   "source": [
    "c = mlflow.tracking.MlflowClient()\n",
    "r = c.get_run(run_id)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.data.params['degree']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
